{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luongvantuit/learn-ai-with-python/blob/master/soft-margin/VD1_BTTH1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y2WgxDulA1Ev"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Random seed for reproducibility\n",
        "np.random.seed(21)\n",
        "\n",
        "# Generate synthetic data\n",
        "means = [[2, 2], [4, 1]]\n",
        "cov = [[0.3, 0.2], [0.2, 0.3]]\n",
        "N = 10\n",
        "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
        "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
        "X1[-1, :] = [2.7, 2]  # Adjust one point for edge case\n",
        "X = np.hstack((X0.T, X1.T))  # Input data (2 x 20)\n",
        "y = np.hstack((np.ones(N), -np.ones(N)))  # Labels (1 x 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkSYlqUCA1Ew",
        "outputId": "60cbe851-3f0e-4cb0-8cbe-f99ea1496c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: Lagrange Dual\n",
            "Weights: [[-5.54276837  2.41628387]]\n",
            "Bias: 9.132906850859555\n",
            "Accuracy: 50.00%\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "\n",
            "Method: Gradient Descent\n",
            "Weights: [[-5.54923267  2.41881653]]\n",
            "Bias: 9.14508377\n",
            "Accuracy: 50.00%\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "\n",
            "Method: Sklearn\n",
            "Weights: [[-5.54202362  2.4156074 ]]\n",
            "Bias: 9.13241559\n",
            "Accuracy: 50.00%\n",
            "Confusion Matrix:\n",
            "[[10  0]\n",
            " [ 0 10]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Predefined weights and biases for three methods\n",
        "w_dual = np.array([[-5.54276837], [2.41628387]])\n",
        "b_dual = 9.132906850859555\n",
        "\n",
        "w_hinge = np.array([[-5.54923267], [2.41881653]])\n",
        "b_hinge = 9.14508377\n",
        "\n",
        "w_sklearn = np.array([[-5.54202362], [2.4156074]])\n",
        "b_sklearn = 9.13241559\n",
        "\n",
        "# Prediction function\n",
        "def predict(X, w, b):\n",
        "    return np.sign(X.T @ w + b)\n",
        "\n",
        "# Accuracy function\n",
        "def accuracy(y_true, y_pred):\n",
        "    return np.mean(y_true == y_pred) * 100\n",
        "\n",
        "# Confusion matrix function\n",
        "def confusion_matrix_report(y_true, y_pred):\n",
        "    return confusion_matrix(y_true, y_pred, labels=[1, -1])\n",
        "\n",
        "# Run predictions and evaluations for each method\n",
        "results = []\n",
        "methods = [\"Lagrange Dual\", \"Gradient Descent\", \"Sklearn\"]\n",
        "weights = [(w_dual, b_dual), (w_hinge, b_hinge), (w_sklearn, b_sklearn)]\n",
        "\n",
        "for method, (w, b) in zip(methods, weights):\n",
        "    y_pred = predict(X, w, b)\n",
        "    acc = accuracy(y, y_pred)\n",
        "    cm = confusion_matrix_report(y, y_pred)\n",
        "    results.append({\n",
        "        \"Method\": method,\n",
        "        \"Weights\": w.T,\n",
        "        \"Bias\": b,\n",
        "        \"Accuracy (%)\": acc,\n",
        "        \"Confusion Matrix\": cm\n",
        "    })\n",
        "\n",
        "# Print results\n",
        "for res in results:\n",
        "    print(f\"Method: {res['Method']}\")\n",
        "    print(f\"Weights: {res['Weights']}\")\n",
        "    print(f\"Bias: {res['Bias']}\")\n",
        "    print(f\"Accuracy: {res['Accuracy (%)']:.2f}%\")\n",
        "    print(f\"Confusion Matrix:\\n{res['Confusion Matrix']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All three methods perform similarly in terms of accuracy for this dataset. However, the Scikit-learn implementation is recommended for most practical use cases due to its ease of use and computational efficiency. Lagrange Dual is suitable for theoretical exploration and understanding the SVM formulation, while Gradient Descent is valuable for custom implementations and cases requiring iterative optimization."
      ],
      "metadata": {
        "id": "TGlHR_QlBsTP"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}